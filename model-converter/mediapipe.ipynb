{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM Conversion\n",
    "Colab logo Run in Colab \tGitHub logo View on GitHub\n",
    "\n",
    "#@title License information { display-mode: \"form\" }\n",
    "#@markdown Copyright 2024 The MediaPipe Authors.\n",
    "#@markdown Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#@markdown\n",
    "#@markdown you may not use this file except in compliance with the License.\n",
    "#@markdown You may obtain a copy of the License at\n",
    "#@markdown\n",
    "#@markdown https://www.apache.org/licenses/LICENSE-2.0\n",
    "#@markdown\n",
    "#@markdown Unless required by applicable law or agreed to in writing, software\n",
    "#@markdown distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#@markdown WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#@markdown See the License for the specific language governing permissions and\n",
    "#@markdown limitations under the License.\n",
    "     \n",
    "\n",
    "# @title Setup { display-mode: \"form\" }\n",
    "# @markdown import ipywidgets\\\n",
    "# @markdown import IPython.display\\\n",
    "# @markdown import os\\\n",
    "# @markdown import huggingface downloader\\\n",
    "# @markdown import mediapipe genai converter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "install_out = widgets.Output()\n",
    "display(install_out)\n",
    "with install_out:\n",
    "  !pip install mediapipe\n",
    "  !pip install torch\n",
    "  !pip install huggingface_hub\n",
    "  import os\n",
    "  from huggingface_hub import hf_hub_download\n",
    "  from mediapipe.tasks.python.genai import converter\n",
    "\n",
    "install_out.clear_output()\n",
    "with install_out:\n",
    "  print(\"Setup done.\")\n",
    "\n",
    "     \n",
    "\n",
    "Output()\n",
    "\n",
    "\n",
    "# @title Run { display-mode: \"form\"}\n",
    "\n",
    "model = widgets.Dropdown(\n",
    "    options=[\"Gemma 2B\",\"Gemma 7B\", \"Falcon 1B\", \"StableLM 3B\", \"Phi 2\"],\n",
    "    value='Gemma 2B',\n",
    "    description='model',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "backend = widgets.Dropdown(\n",
    "    options=[\"cpu\", \"gpu\"],\n",
    "    value='cpu',\n",
    "    description='backend',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "token = widgets.Password(\n",
    "    value='',\n",
    "    placeholder='huggingface token',\n",
    "    description='HF token:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "options_mapping = {\n",
    "              'Gemma 2B': ['cpu', 'gpu'],\n",
    "              'Gemma 7B': ['gpu'],\n",
    "              'Falcon 1B': ['cpu', 'gpu'],\n",
    "              'StableLM 3B': ['cpu', 'gpu'],\n",
    "              'Phi 2': ['cpu', 'gpu']\n",
    "}\n",
    "\n",
    "def on_use_gpu(change):\n",
    "  selected_value = change['new']\n",
    "\n",
    "  if selected_value in options_mapping:\n",
    "    backend.options = options_mapping[selected_value]\n",
    "    backend.value = options_mapping[selected_value][0]\n",
    "  else:\n",
    "    token.options = []\n",
    "    token.value = None\n",
    "\n",
    "def on_change_model(change):\n",
    "  selected_values = ['Gemma 2B','Gemma 7B']\n",
    "\n",
    "  if change['new'] in selected_values:\n",
    "    token.layout.display = 'flex'\n",
    "  else:\n",
    "    token.layout.display = 'none'\n",
    "\n",
    "model.observe(on_change_model, names='value')\n",
    "model.observe(on_use_gpu, names='value')\n",
    "\n",
    "\n",
    "display(model)\n",
    "display(backend)\n",
    "\n",
    "token_description = widgets.Output()\n",
    "with token_description:\n",
    "  print(\"Huggingface token needed for gated model (e.g. Gemma)\")\n",
    "  print(\"You can get it from https://huggingface.co/settings/tokens\")\n",
    "\n",
    "display(token_description)\n",
    "display(token)\n",
    "\n",
    "\n",
    "\n",
    "def gemma2b_download(token):\n",
    "  REPO_ID = \"google/gemma-2b-it\"\n",
    "  FILENAMES = [\"tokenizer.json\", \"tokenizer_config.json\", \"model-00001-of-00002.safetensors\", \"model-00002-of-00002.safetensors\"]\n",
    "  os.environ['HF_TOKEN'] = token\n",
    "  with out:\n",
    "    for filename in FILENAMES:\n",
    "      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir=\"./gemma-2b-it\")\n",
    "\n",
    "def gemma7b_download(token):\n",
    "  REPO_ID = \"google/gemma-1.1-7b-it\"\n",
    "  FILENAMES = [\"tokenizer.json\", \"tokenizer_config.json\", \"model-00001-of-00004.safetensors\", \"model-00002-of-00004.safetensors\", \"model-00003-of-00004.safetensors\", \"model-00004-of-00004.safetensors\"]\n",
    "  os.environ['HF_TOKEN'] = token\n",
    "  with out:\n",
    "    for filename in FILENAMES:\n",
    "      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir=\"./gemma-1.1-7b-it\")\n",
    "\n",
    "def falcon_download():\n",
    "  REPO_ID = \"tiiuae/falcon-rw-1b\"\n",
    "  FILENAMES = [\"tokenizer.json\", \"tokenizer_config.json\", \"pytorch_model.bin\"]\n",
    "  with out:\n",
    "    for filename in FILENAMES:\n",
    "      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir=\"./falcon-rw-1b\")\n",
    "\n",
    "def stablelm_download():\n",
    "  REPO_ID = \"stabilityai/stablelm-3b-4e1t\"\n",
    "  FILENAMES = [\"tokenizer.json\", \"tokenizer_config.json\", \"model.safetensors\"]\n",
    "  with out:\n",
    "    for filename in FILENAMES:\n",
    "      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir=\"./stablelm-3b-4e1t\")\n",
    "\n",
    "def phi2_download():\n",
    "  REPO_ID = \"microsoft/phi-2\"\n",
    "  FILENAMES = [\"tokenizer.json\", \"tokenizer_config.json\", \"model-00001-of-00002.safetensors\", \"model-00002-of-00002.safetensors\"]\n",
    "  with out:\n",
    "    for filename in FILENAMES:\n",
    "      hf_hub_download(repo_id=REPO_ID, filename=filename, local_dir=\"./phi-2\")\n",
    "\n",
    "\n",
    "def gemma2b_convert_config(backend):\n",
    "  input_ckpt = '/content/gemma-2b-it/'\n",
    "  vocab_model_file = '/content/gemma-2b-it/'\n",
    "  output_dir = '/content/intermediate/gemma-2b-it/'\n",
    "  output_tflite_file = f'/content/converted_models/gemma_{backend}.bin'\n",
    "  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='GEMMA_2B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)\n",
    "\n",
    "def gemma7b_convert_config(backend):\n",
    "  input_ckpt = '/content//gemma-1.1-7b-it/'\n",
    "  vocab_model_file = '/content//gemma-1.1-7b-it/'\n",
    "  output_dir = '/content/intermediate//gemma-1.1-7b-it/'\n",
    "  output_tflite_file = f'/content/converted_models/gemma_{backend}.bin'\n",
    "  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='GEMMA_7B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)\n",
    "\n",
    "def falcon_convert_config(backend):\n",
    "  input_ckpt = '/content/falcon-rw-1b/pytorch_model.bin'\n",
    "  vocab_model_file = '/content/falcon-rw-1b/'\n",
    "  output_dir = '/content/intermediate/falcon-rw-1b/'\n",
    "  output_tflite_file = f'/content/converted_models/falcon_{backend}.bin'\n",
    "  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='pytorch', model_type='FALCON_RW_1B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)\n",
    "\n",
    "def stablelm_convert_config(backend):\n",
    "  input_ckpt = '/content/stablelm-3b-4e1t/'\n",
    "  vocab_model_file = '/content/stablelm-3b-4e1t/'\n",
    "  output_dir = '/content/intermediate/stablelm-3b-4e1t/'\n",
    "  output_tflite_file = f'/content/converted_models/stablelm_{backend}.bin'\n",
    "  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='STABLELM_4E1T_3B', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)\n",
    "\n",
    "\n",
    "def phi2_convert_config(backend):\n",
    "  input_ckpt = '/content/phi-2'\n",
    "  vocab_model_file = '/content/phi-2/'\n",
    "  output_dir = '/content/intermediate/phi-2/'\n",
    "  output_tflite_file = f'/content/converted_models/phi2_{backend}.bin'\n",
    "\n",
    "  return converter.ConversionConfig(input_ckpt=input_ckpt, ckpt_format='safetensors', model_type='PHI_2', backend=backend, output_dir=output_dir, combine_file_only=False, vocab_model_file=vocab_model_file, output_tflite_file=output_tflite_file)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "  try:\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "      print(\"Downloading model ...\")\n",
    "    button.description = \"Downloading ...\"\n",
    "    button.disabled = True\n",
    "    model.disabled = True\n",
    "    backend.disabled = True\n",
    "\n",
    "    if model.value == 'Gemma 2B':\n",
    "      gemma2b_download(token.value)\n",
    "    elif model.value == 'Gemma 7B':\n",
    "      gemma7b_download(token.value)\n",
    "    elif model.value == 'Falcon 1B':\n",
    "      falcon_download()\n",
    "    elif model.value == 'StableLM 3B':\n",
    "      stablelm_download()\n",
    "    elif model.value == 'Phi 2':\n",
    "      phi2_download()\n",
    "    else:\n",
    "      raise Exception(\"Invalid model\")\n",
    "\n",
    "    with out:\n",
    "      print(\"Done\")\n",
    "      print(\"Converting model ...\")\n",
    "\n",
    "    button.description = \"Converting ...\"\n",
    "\n",
    "    if model.value == 'Gemma 2B':\n",
    "      config = gemma2b_convert_config(backend.value)\n",
    "    elif model.value == 'Gemma 7B':\n",
    "      config = gemma7b_convert_config(backend.value)\n",
    "    elif model.value == 'Falcon 1B':\n",
    "      config = falcon_convert_config(backend.value)\n",
    "    elif model.value == 'StableLM 3B':\n",
    "      config = stablelm_convert_config(backend.value)\n",
    "    elif model.value == 'Phi 2':\n",
    "      config = phi2_convert_config(backend.value)\n",
    "    else:\n",
    "      with out:\n",
    "        raise Exception(\"Invalid model\")\n",
    "      return\n",
    "\n",
    "    with out:\n",
    "      converter.convert_checkpoint(config)\n",
    "      print(\"Done\")\n",
    "\n",
    "    button.description = \"Start Conversion\"\n",
    "    button.disabled = False\n",
    "    model.disabled = False\n",
    "    backend.disabled = False\n",
    "\n",
    "  except Exception as e:\n",
    "    button.description = \"Start Conversion\"\n",
    "    button.disabled = False\n",
    "    model.disabled = False\n",
    "    backend.disabled = False\n",
    "    with out:\n",
    "      print(e)\n",
    "\n",
    "\n",
    "button = widgets.Button(description=\"Start Conversion\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(out)\n",
    "\n",
    "print(\"\\nNotice: Converted models are saved under ./converted_models\")\n",
    "     \n",
    "\n",
    "Dropdown(description='model', options=('Gemma 2B', 'Falcon 1B', 'StableLM 3B', 'Phi 2'), value='Gemma 2B')\n",
    "\n",
    "Dropdown(description='backend', options=('cpu', 'gpu'), value='cpu')\n",
    "\n",
    "Output()\n",
    "\n",
    "Password(description='HF token:', placeholder='huggingface token')\n",
    "\n",
    "Button(description='Start Conversion', style=ButtonStyle())\n",
    "\n",
    "Output(layout=Layout(border='1px solid black'))\n",
    "\n",
    "Notice: Converted models are saved under ./converted_models\n",
    "\n",
    "\n",
    "# @title Save to google drive { display-mode: \"form\"}\n",
    "google_drive_directory = \"converted_models\" #@param {type:\"string\"}\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Copying models ...\")\n",
    "!mkdir -p /content/drive/MyDrive/\n",
    "google_drive_directory\n",
    "print(\"Done\")\n",
    "     \n",
    "\n",
    "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
    "copying models ...\n",
    "done\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
